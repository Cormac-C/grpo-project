{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df7262ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66f65ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cormaccureton/mambaforge/envs/grpo-proj/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66db916b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup module path for local imports\n",
    "project_root = os.path.abspath(os.path.join(\"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Importing the necessary modules\n",
    "from grpo import grpo_iteration, evaluate_policy, sample_outputs, compute_log_probs, compute_log_probs_2\n",
    "from dataset.countdown_utils import batch_compute_metrics\n",
    "from dataset.countdown_dataloader import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72872dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7c577af",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f42a9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    prompts = [item[\"prompt\"] for item in batch]\n",
    "    targets = torch.tensor([item[\"target\"] for item in batch])\n",
    "    numbers = [torch.tensor(item[\"numbers\"]) for item in batch]\n",
    "    # Pad the numbers to the same length\n",
    "    padded_numbers = pad_sequence(\n",
    "        numbers, batch_first=True, padding_value=0\n",
    "    )  # Pad with zeros\n",
    "    return {\n",
    "        \"prompt\": prompts,\n",
    "        \"target\": targets,\n",
    "        \"numbers\": padded_numbers,\n",
    "    }\n",
    "\n",
    "# Load dataset\n",
    "dataset = Countdown(\n",
    "    json_path=\"../data/small-scale/countdown.json\",\n",
    "    model_type=\"instruct\"\n",
    ")\n",
    "data_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1230329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# Initialize the model with empty weights if needed\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55140dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "G=2\n",
    "max_new_tokens=256\n",
    "temperature=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6d5ccc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(data_loader))\n",
    "\n",
    "output_ids, generated_ids, outputs = sample_outputs(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    batch[\"prompt\"],\n",
    "    G,\n",
    "    max_new_tokens,\n",
    "    temperature,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd5cf1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output IDs: torch.Size([2, 2, 391])\n",
      "Generated IDs: torch.Size([2, 2, 256])\n",
      "Outputs: [[\"  First, we'll assign the numbers to their corresponding operators. So now here we are:\\n( 1 + 4 ) / 24</think> </think>\\nSo first we calculate 1 + 4 = 5, and then divide 5 by 24, which gives us 5/24</think> </think>\\nNext, we'll calculate 5/24 times 1/24 (which is basically the reciprocal of 24), so we get 5/576</think> </think>\\nFinally, we'll divide 5/576 by 24. This gives us 1/2768</think> </think>\\nNow, we should replace the numbers back in the order of the given list:\\n( 1 + 4 ) / 24 * ( 1 / 24 + 5 / 576 ) / 2\\nWe already calculated ( 1 + 4 ) and ( 1 / 24 + 5 / 576 ) for us, and the result is 2  <answer> (1 + 4 ) / 24 * ( 1 / 24 + 5\", 'First, I will define the equation to be 21. </think>\\nThink:\\n\\t4 + 24 - 1 = 21\\n<br>\\nHere, I used the numbers 4 and 24 and then did the addition, subtraction, and division step by step in the equation.\\nThink:\\n\\t4 + 1 + 24 - 1 = 21\\n<br>\\nHere, I used the same numbers to calculate the final result of the equation.\\nThink:\\n\\t4 + 24 - 1 - 1 = 21\\n<br>\\nHere, I used the numbers 1 and -1, then subtracted them to get the answer.\\nThink:\\n\\t24 + 24 = 48\\nThink:\\n\\t48 - 1 = 47\\nThink:\\n\\t47 + 1 = 48\\nThink:\\n\\t24 - 1 = 23\\nThink:\\n\\t48 - 23 = 25\\nThink:\\n\\t24 - 1 = 23\\nThink:\\n\\t24 - 1 = 23\\nThink:\\n\\t24 - 1 = 23\\nThink:\\n\\t24 - 1'], ['15 - 7 = 8</think> <answer>8</answer>', ' (1 + 2) / 3 </think>\\nStep 1: Perform addition.\\n(1 + 2) = 3\\nSo, we have 3 รท 3 = ?\\nStep 2: Perform division.\\n3 รท 3 = 1\\nMy final step is to return the final answer.']]\n"
     ]
    }
   ],
   "source": [
    "print(\"Output IDs:\", output_ids.shape)\n",
    "print(\"Generated IDs:\", generated_ids.shape)\n",
    "print(\"Outputs:\", outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e56d3041",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 18:58:55,469 - INFO - Attention mask shape: torch.Size([4, 391])\n",
      "2025-04-16 18:58:55,469 - INFO - Amount of tokens: 1133\n",
      "2025-04-16 18:59:10,587 - INFO - Generated logits shape: torch.Size([4, 255, 151936])\n",
      "2025-04-16 18:59:11,053 - INFO - Generated IDs shape: torch.Size([2, 2, 255])\n"
     ]
    }
   ],
   "source": [
    "log_probs = compute_log_probs(\n",
    "    policy=model,\n",
    "    tokenizer=tokenizer,\n",
    "    query_batch=batch[\"prompt\"],\n",
    "    generated_ids=generated_ids,\n",
    "    temperature=temperature,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e110afcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Probs: torch.Size([2, 2, 255])\n",
      "Log Probs: tensor([[[ -5.4844, -14.3594, -13.4062,  ..., -14.0781, -11.0391,  -8.8125],\n",
      "         [-19.2188, -13.3516, -10.2422,  ..., -12.2109, -10.3359, -13.1562]],\n",
      "\n",
      "        [[ -6.1719,  -7.1328,  -7.7930,  ..., -17.6719, -17.3125, -17.3125],\n",
      "         [ -6.9883,  -9.0234, -10.8594,  ..., -16.4062, -16.7656, -16.2656]]],\n",
      "       dtype=torch.float16, grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Log Probs:\", log_probs.shape)\n",
    "print(\"Log Probs:\", log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21bcde06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 19:02:36,551 - INFO - Output IDs shape: torch.Size([2, 2, 391])\n",
      "2025-04-16 19:02:36,553 - INFO - Generated IDs shape: torch.Size([2, 2, 256])\n",
      "2025-04-16 19:02:36,562 - INFO - Output IDs reshaped: torch.Size([4, 391])\n",
      "2025-04-16 19:02:52,390 - INFO - Logits shape: torch.Size([4, 390, 151936])\n",
      "2025-04-16 19:02:52,484 - INFO - Output IDs shape: torch.Size([4, 390])\n",
      "2025-04-16 19:02:53,347 - INFO - Log probs shape: torch.Size([4, 390])\n",
      "2025-04-16 19:02:53,355 - INFO - Query length: 134\n",
      "2025-04-16 19:02:53,392 - INFO - Log probs reshaped: torch.Size([2, 2, 256])\n"
     ]
    }
   ],
   "source": [
    "log_probs_2 = compute_log_probs_2(\n",
    "    policy=model,\n",
    "    tokenizer=tokenizer,\n",
    "    query_batch=batch[\"prompt\"],\n",
    "    output_ids=output_ids,\n",
    "    generated_ids=generated_ids,\n",
    "    temperature=temperature,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3f961fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Probs 2: torch.Size([2, 2, 256])\n",
      "Log Probs 2: tensor([[[-2.3008e+00, -6.6875e+00, -1.0992e-01,  ..., -2.9259e-03,\n",
      "          -9.7609e-04, -3.8986e-03],\n",
      "         [-2.3164e+00, -1.0205e-01, -1.1035e+00,  ..., -2.9831e-02,\n",
      "          -9.7609e-04, -2.1255e-02]],\n",
      "\n",
      "        [[-2.2812e+00, -5.9062e+00, -2.6641e+00,  ..., -1.7672e+01,\n",
      "          -1.7312e+01, -1.7312e+01],\n",
      "         [-3.7344e+00, -5.1392e-02, -2.6587e-01,  ..., -1.6406e+01,\n",
      "          -1.6766e+01, -1.6266e+01]]], dtype=torch.float16,\n",
      "       grad_fn=<ReshapeAliasBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Log Probs 2:\", log_probs_2.shape)\n",
    "print(\"Log Probs 2:\", log_probs_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grpo-proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
