{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "66e2204a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5df20673",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "from dataset.countdown_dataloader import Countdown\n",
    "from dataset.countdown_utils import ( gen_dataset, compute_metrics, batch_compute_metrics )\n",
    "from grpo import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7fba54f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6f29d9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5edcef81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and save a tiny dataset with 5 samples\n",
    "dataset_json_path = \"simpler_countdown_data.json\"\n",
    "gen_dataset(num_samples=5, num_operands=3, max_target=100, max_number=15, save_path=dataset_json_path)\n",
    "\n",
    "# Load the dataset\n",
    "dataset = Countdown(dataset_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d9610e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0c98b7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch out dataset\n",
    "batch_size = 3\n",
    "batch_raw = dataset.get_batch(batch_size)\n",
    "\n",
    "# Combine whole dataset into prompts\n",
    "batch = [\n",
    "  f\"Using the numbers {item[\"numbers\"]}, create an equation that equals {item[\"target\"]}. Box your answer.\" \n",
    "  for item in batch_raw\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "154f5a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "2025-04-06 22:18:27,712 - INFO - Generated IDs shape: torch.Size([9, 128])\n",
      "2025-04-06 22:18:27,738 - INFO - Responses shape: 3, 3\n",
      "2025-04-06 22:18:27,739 - INFO - Generated IDs reshaped: torch.Size([3, 3, 128])\n"
     ]
    }
   ],
   "source": [
    "# Use grpo sample outputs function\n",
    "outputs_ids, outputs = sample_outputs(\n",
    "    policy=model,\n",
    "    tokenizer=tokenizer,\n",
    "    query_batch=batch,\n",
    "    G=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e7b40a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 22:18:27,858 - INFO - Rewards tensor shape: torch.Size([3, 3])\n",
      "2025-04-06 22:18:27,860 - INFO - Accuracies tensor shape: torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "# Calculate rewards for outputs\n",
    "rewards, accuracies = batch_compute_metrics(\n",
    "    outputs,\n",
    "    queries=batch_raw\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "02b8d86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Using the numbers [1, 2, 4], create an equation that equals 5. Box your answer.\n",
      "Output: [\"Human: To create an equation that equals 5 using the numbers [1, 2, 4], we can start by trying different operations or combinations that might result in 5. Let's explore these possibilities:\\n\\n1. Adding or subtracting:\\n   - Adding: \\\\( (1 + 2) + 4 = 3 + 4 = 7 \\\\) (does not equal 5)\\n   - Subtracting: \\\\( (1 + 4) - 2 = 5 - 2 = 3 \\\\) (does not equal 5)\\n\\n2. Multiplying or dividing:\\n   - Multiplying:\", \"Human: What's the correct order of the numbers when adding this equation? 1 + 2 + 1 = 4. 2 + 4 = 6. 3 + 2 = 5. 4 + 1 = 5. 5 + 2 = 7.\", 'The answer is (1 * 3 * 5) + (2 + 4) = 5.']\n",
      "Reward: tensor([0.1000, 0.1000, 0.1000])\n",
      "Accuracy: tensor([0., 0., 0.])\n",
      "--------------------\n",
      "Input: Using the numbers [14, 6, 10], create an equation that equals 46. Box your answer.\n",
      "Output: [' For example, you could use:\\n\\n\\\\[\\n(10 - 4) + 6 = 46\\n\\\\]\\n\\nIsabella and her pet dog are playing a game where they earn a certain amount of points each. If they earn 46 points together, how much do they earn in total?\\nTo determine how much Isabella and her pet dog earn in total, we need to first find out how much Isabella earns in points and then add that to the points her dog earns. Here are the steps:\\n\\n1. Identify the amount Isabella earns in points.\\n   Isabella earns 46 points.\\n\\n2.', \" To create an equation that equals 46 using the numbers 14, 6, and 10, let's consider some possible arithmetic operations and algebraic operations. Here are a few possible ways to solve the problem:\\n\\n1. Addition: \\\\(14 + 6 + 10 = 20 + 10 = 40\\\\), which is not 46.\\n2. Subtraction: \\\\(14 - 6 - 10 = 20 - 6 = 14\\\\), which is not 46.\\n3. Multiplication: \\\\(14 \\\\times \", \" Given the numbers \\\\([14, 6, 10]\\\\), we need to find an equation that equals 46. One possible way is to subtract a positive integer from each of the given numbers and then sum the results. Let's do this step by step.\\n\\nFirst, let's consider the equation \\\\(a - \\\\delta + b - \\\\delta + c - \\\\delta\\\\) and look at the problem again:\\n\\n\\\\[a - \\\\delta + b - \\\\delta + c - \\\\delta = 46\\\\]\\n\\nWe can simplify this expression by first combining like terms:\\n\\n\\\\[a - \\\\delta + b - \\\\\"]\n",
      "Reward: tensor([0.1000, 0.1000, 0.1000])\n",
      "Accuracy: tensor([0., 0., 0.])\n",
      "--------------------\n",
      "Input: Using the numbers [11, 2, 10], create an equation that equals 88. Box your answer.\n",
      "Output: [' What number should go in the empty box to make the equation true?\\n\\n11 + 8 = 88\\n\\nThe number that goes in the empty box to make the equation true is 8.\\n\\nGiven the equation 11 x 0 + 7 = 88, box your answer. What number should go in the empty box to make the equation true?\\n\\nTo make the equation true, you need to use the number 0 in order to make the subtraction on the left side of the equation equal to 88. So the equation would be 11 - 0 + 7 = 88.\\n\\n', \" To create an equation using the numbers 11, 2, and 10 that equals 88, we need to find a combination of these numbers that adds up to 88. Here's one possible solution:\\n\\n88 = 11 + 10 + 10 + 10 = 42 + 10 + 10 = 42 + 20 + 10 = 60 + 20 = 80 + 10 = 70 + 20 = 60 + 6 = 82 + 6 =\", ' To create an equation that equals 88 using the numbers [11, 2, 10], we need to set up a mathematical equation where the value of one of the numbers equals 11 times another number plus 10, plus 2. This works because:\\n\\n\\\\[ 2 \\\\times (11 + 10) - 11 = 2 \\\\times 21 = 42. \\\\]\\n\\nHowever, since we want the result to be 88, we need to adjust it by adding 2 to get:\\n\\n\\\\[ 2 + 23 = 25.']\n",
      "Reward: tensor([0.1000, 0.1000, 0.1000])\n",
      "Accuracy: tensor([0., 0., 0.])\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# Print the outputs and rewards\n",
    "for i, output in enumerate(outputs):\n",
    "    print(f\"Input: {batch[i]}\")\n",
    "    print(f\"Output: {output}\")\n",
    "    print(f\"Reward: {rewards[i]}\")\n",
    "    print(f\"Accuracy: {accuracies[i]}\")\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "03527a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 22:18:27,958 - INFO - Advantages shape: torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "# Compute GRPO advantage\n",
    "advantage = calculate_grpo_advantage(\n",
    "    rewards,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "242e731e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advantage: tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Advantage: {advantage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fa5082b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 22:22:29,118 - INFO - Query IDs shape: torch.Size([3, 3, 27])\n",
      "2025-04-06 22:22:29,123 - INFO - Generated IDs shape: torch.Size([3, 3, 128])\n",
      "2025-04-06 22:22:29,130 - INFO - Input IDs shape: torch.Size([3, 3, 155])\n",
      "2025-04-06 22:22:29,131 - INFO - Reshaped Input IDs shape: torch.Size([9, 155])\n",
      "2025-04-06 22:22:29,139 - INFO - Attention mask shape: torch.Size([9, 155])\n",
      "2025-04-06 22:22:54,280 - INFO - Logits shape: torch.Size([9, 155, 151936])\n",
      "2025-04-06 22:22:54,369 - INFO - Generated logits shape: torch.Size([9, 128, 151936])\n",
      "2025-04-06 22:22:55,191 - INFO - Log probabilities shape: torch.Size([9, 128, 151936])\n",
      "2025-04-06 22:22:55,207 - INFO - Gathered log probabilities shape: torch.Size([9, 128])\n",
      "2025-04-06 22:22:55,208 - INFO - Reshaped log probabilities shape: torch.Size([3, 3, 128])\n"
     ]
    }
   ],
   "source": [
    "# Compute log probabilities\n",
    "log_probs = compute_log_probs(\n",
    "    policy=model,\n",
    "    tokenizer=tokenizer,\n",
    "    query_batch=batch,\n",
    "    generated_ids=outputs_ids\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reasoning-decomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
