{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "66e2204a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5df20673",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "from dataset.countdown_dataloader import Countdown\n",
    "from dataset.countdown_utils import ( gen_dataset, compute_metrics, batch_compute_metrics )\n",
    "from grpo import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7fba54f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6f29d9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5edcef81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and save a tiny dataset with 5 samples\n",
    "dataset_json_path = \"simpler_countdown_data.json\"\n",
    "gen_dataset(num_samples=5, num_operands=3, max_target=100, max_number=15, save_path=dataset_json_path)\n",
    "\n",
    "# Load the dataset\n",
    "dataset = Countdown(dataset_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d9610e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0c98b7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch out dataset\n",
    "batch_size = 3\n",
    "batch_raw = dataset.get_batch(batch_size)\n",
    "\n",
    "# Combine whole dataset into prompts\n",
    "batch = [\n",
    "  f\"Using the numbers {item[\"numbers\"]}, create an equation that equals {item[\"target\"]}. Box your answer.\" \n",
    "  for item in batch_raw\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "154f5a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 21:33:14,909 - INFO - Generated IDs shape: torch.Size([9, 128])\n",
      "2025-04-06 21:33:14,935 - INFO - Responses shape: 3, 3\n",
      "2025-04-06 21:33:14,936 - INFO - Generated IDs reshaped: torch.Size([3, 3, 128])\n"
     ]
    }
   ],
   "source": [
    "# Use grpo sample outputs function\n",
    "outputs_ids, outputs = sample_outputs(\n",
    "    policy=model,\n",
    "    tokenizer=tokenizer,\n",
    "    d_b=batch,\n",
    "    G=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e7b40a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 21:33:15,050 - INFO - Rewards tensor shape: torch.Size([3, 3])\n",
      "2025-04-06 21:33:15,051 - INFO - Accuracies tensor shape: torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "# Calculate rewards for outputs\n",
    "rewards, accuracies = batch_compute_metrics(\n",
    "    outputs,\n",
    "    queries=batch_raw\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "02b8d86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Using the numbers [1, 2, 4], create an equation that equals 5. Box your answer.\n",
      "Output: [\"How can you use this same box to create a different equation that equals 5, but does not use the letter 'b'. What's the box? To solve this problem, let's first understand the context of the exercise. It seems like we have a set of numbers 1, 2, and 4 and we need to use the numbers in a way that we can create an equation that equals 5 using the given set and without using the letter 'b'.\\n\\n### Step-by-Step Solution:\\n\\n1. **Identify how we can form an equation:**\\n   - We need to use the numbers 1, \", 'Human: (1 * 2) + 4 = (5)', 'Human: It is not possible to create an equation using the numbers [1, 2, 4] that equals 5 due to the mathematical properties of numbers and logic. This is because you are only using an integer 1, 2, and 4, and you cannot form any other combination of these numbers that will equal 5.\\n\\nThe correct way to use the numbers [1, 2, 4] to create an equation equal to 5 would be to first calculate 5 minus the sum of the other two numbers. This sum would then be set aside, and one of the remaining numbers would then equal']\n",
      "Reward: tensor([0.0000, 0.1000, 0.0000])\n",
      "Accuracy: tensor([0., 0., 0.])\n",
      "--------------------\n",
      "Input: Using the numbers [14, 6, 10], create an equation that equals 46. Box your answer.\n",
      "Output: [\" Certainly! Here's one way to create an equation using the numbers 14, 6, and 10 that equals 46:\\n\\n46 = 10 - (14 - 3)\\n\\nHere's how it works:\\n\\n1. Start by subtracting 3 from 14 (the number that gets removed from 14):\\n   10 - 3 = 7\\n\\n2. Now, subtract this result from 14 (the number that gets removed from 14):\\n   14 - 7 = 7\\n\\n3. Finally, subtract 10 from this result\", \" Sure, I'll break down the problem into two parts. First, I'll explore possible ways to form the equation using the digits 1, 2, 3, 4, 5, and 6. Then I'll present all valid expressions for any single five-digit number. However, since it's a simple arithmetic equation like adding up digits, it doesn't fit the multiple-digit rules. For instance, there don't exist five-digit sums of two different digits: (1+4) + (2+4) + (3+6) + (5+5) + (6+6) = \", '  [X+(14-X)(6-X)]\\n[46] = 60 is wrong because 60 * 2 = 120. You have to multiply the (14-X)(6-X) together. The numbers squared are -3.44, 160.4 and 264.7. - You have to multiply the -3.44 by -1 to make 4.32  (11*3.44) or  -4.34 * -3.44 * -1. 5. A = 1']\n",
      "Reward: tensor([0.1000, 0.1000, 0.1000])\n",
      "Accuracy: tensor([0., 0., 0.])\n",
      "--------------------\n",
      "Input: Using the numbers [11, 2, 10], create an equation that equals 88. Box your answer.\n",
      "Output: [' [11 - (2 + 10)] = 88.\\n\\nThe equation that equals 88 out of the numbers [11, 2, 10] is given by [11 - (2 + 10)] = 88.', \" To create an equation that sums up to 88 using the numbers [11, 2, 10], we need to find a combination of these digits that adds up to 88. Here's how we can solve this problem step-by-step with Python code.\\n\\n1. Start by examining all possible combinations of the digits [11, 2, 10] and try to find a combination that adds up to 88.\\n2. Use a brute-force approach to find the correct combination.\\n\\nLet's write the code to find the solution.\\n```python\\n# Define the numbers\\nnumbers = [1\", ' Here is a numerical equation using the numbers [11, 2, 10] that equals 88:\\n\\n11 * 11 - 10 = 88\\n\\nThis equation can be simplified to:\\n\\n121 - 10 = 88\\n\\n91 = 88\\n\\nThe resulting equation is not an exact match of 88, but rather an approximation close enough to meet the requirement.']\n",
      "Reward: tensor([0.1000, 0.1000, 0.1000])\n",
      "Accuracy: tensor([0., 0., 0.])\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# Print the outputs and rewards\n",
    "for i, output in enumerate(outputs):\n",
    "    print(f\"Input: {batch[i]}\")\n",
    "    print(f\"Output: {output}\")\n",
    "    print(f\"Reward: {rewards[i]}\")\n",
    "    print(f\"Accuracy: {accuracies[i]}\")\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "03527a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 21:34:55,572 - INFO - Advantages shape: torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "# Compute GRPO advantage\n",
    "advantage = calculate_grpo_advantage(\n",
    "    rewards,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "242e731e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advantage: tensor([[-0.5774,  1.1547, -0.5774],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Advantage: {advantage}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reasoning-decomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
