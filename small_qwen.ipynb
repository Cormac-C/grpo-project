{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66e2204a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5df20673",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import logging\n",
    "\n",
    "from dataset.countdown_dataloader import Countdown\n",
    "from dataset.countdown_utils import ( gen_dataset, batch_compute_metrics )\n",
    "from grpo import grpo_iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fba54f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f29d9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5edcef81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and save a really simple version of the countdown dataset\n",
    "dataset_json_path = \"simpler_countdown_data.json\"\n",
    "gen_dataset(num_samples=10, num_operands=2, max_target=10, max_number=10, save_path=dataset_json_path)\n",
    "\n",
    "# Load the dataset\n",
    "dataset = Countdown(dataset_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9610e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# Initialize the model with empty weights if needed\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c98b7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch raw:\n",
      "[{'numbers': [tensor(6), tensor(2)], 'target': tensor(4)}, {'numbers': [tensor(2), tensor(7)], 'target': tensor(9)}, {'numbers': [tensor(5), tensor(2)], 'target': tensor(7)}]\n"
     ]
    }
   ],
   "source": [
    "# Batch out dataset\n",
    "batch_size = 3\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "batch = next(iter(dataloader))\n",
    "\n",
    "prompt_batch = batch[\"prompt\"]\n",
    "\n",
    "# Transform batch numbers and target into list of dictionaries\n",
    "# This is slightly hacky, might look at instead reworking the reward model to deal with tensors\n",
    "batch_numbers = list(map(list, zip(*batch[\"numbers\"])))\n",
    "batch_target = batch[\"target\"]\n",
    "\n",
    "raw_values_batch = [{'numbers': numbers, 'target': target} for numbers, target in zip(batch_numbers, batch_target)]\n",
    "print(\"Batch raw:\")\n",
    "print(raw_values_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c99518bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722e35d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "2025-04-07 15:48:24,411 - INFO - outputs: [[\" Let's define a box to keep track of potential outcomes. We'll write helper functions to iterate over possible values and compute whether adding the current 6 and current 2 will yield 4. If this is true, we add the box identifier into our collection, return the first successful outcome, or return an empty string\", \" To create an equation that equals 4 using the numbers 6 and 2, we need to explore different combinations of these numbers. Let's solve it step by step:\\n\\n1. Start with the given numbers: 6 and 2.\\n2. Consider the possible ways to combine these numbers or add them using basic\", ' 62 + 2 + 4.'], [' The equation that equals 9 using the numbers [2, 7] is: 2 + 7 = 9.', ' Here it is:\\n2 x 2 + 7 x 7 = 9.', ' To create an equation that equals 9 using the numbers [2, 7] as variables, we can use the fact that their squares are each 49 (since \\\\(2^2 = 4\\\\) and \\\\(7^2 = 49\\\\)). However, we need to find a way to get'], [' Sure, here is the equation and the answer as follows:\\n\\n\\\\[ \\\\boxed{2 + 5} \\\\]\\n\\nThe answer is 7.', ' One way to create an equation that equals 7 with the given numbers [5, 2] is:\\n\\n{eq}(5+2) + (2-5) = 7 \\n{/eq}\\n\\nThis equation says that the sum of 5 and 2 is equal to 7 plus the difference of ', ' 5 + 2 = 7']]\n",
      "2025-04-07 15:48:24,415 - INFO - queries: [{'numbers': [tensor(6), tensor(2)], 'target': tensor(4)}, {'numbers': [tensor(2), tensor(7)], 'target': tensor(9)}, {'numbers': [tensor(5), tensor(2)], 'target': tensor(7)}]\n",
      "2025-04-07 15:48:24,418 - INFO - Average Accuracy: 0.2222222238779068\n",
      "2025-04-07 15:48:35,277 - INFO - Update iteration: 1/3\n",
      "2025-04-07 15:48:41,416 - INFO - Loss: 9.934107758624577e-09\n"
     ]
    }
   ],
   "source": [
    "updated_policy = grpo_iteration(\n",
    "    query_batch_prompts=prompt_batch,\n",
    "    query_batch_raw=raw_values_batch,\n",
    "    policy_model=model,\n",
    "    reference_model=model,\n",
    "    reward_model=batch_compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    optimizer=optimizer,\n",
    "    G=3,\n",
    "    eps=0.1,\n",
    "    beta=0.05, \n",
    "    mu=3,\n",
    "    max_new_tokens=64\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grpo-proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
