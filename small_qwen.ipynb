{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "66e2204a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5df20673",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "from dataset.countdown_dataloader import Countdown\n",
    "from dataset.countdown_utils import ( gen_dataset, compute_metrics )\n",
    "from grpo import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7fba54f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6f29d9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5edcef81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and save a tiny dataset with 5 samples\n",
    "dataset_json_path = \"simpler_countdown_data.json\"\n",
    "gen_dataset(num_samples=5, num_operands=3, max_target=100, max_number=15, save_path=dataset_json_path)\n",
    "\n",
    "# Load the dataset\n",
    "dataset = Countdown(dataset_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d9610e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0c98b7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine whole dataset into prompts\n",
    "batch = [\n",
    "  f\"Using the numbers {item[\"numbers\"]}, create an equation that equals {item[\"target\"]}. Box your answer.\" \n",
    "  for item in dataset\n",
    "  ]\n",
    "\n",
    "smaller_batch = batch[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154f5a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "2025-04-06 18:48:39,486 - INFO - Responses shape: 2, 3\n",
      "2025-04-06 18:48:39,491 - INFO - Log probabilities shape: torch.Size([2, 3, 128])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Human: It is incorrect to assume that a linear combination of two numbers will always equal another number. For example, using the numbers {2,5} we can create\\n2 * 4 + 5 = 14\\n2 + 5 * 4 = 26.\\nSo if we were looking for a solution to the equation 2 + x = 5, this is the answer.', 'Human: The sum of the two middle numbers equals 3 plus the quotient of the three numbers 1 divided by 2.\\nTherefore, the answer is: \\\\( 3 = 1 + \\\\frac{4}{2} \\\\).', \"(Do not use exponents, square roots and parentheses. Use only addition, subtraction, multiplication, division and parentheses.)\\n\\nWhat's the lowest possible value for the second number?\\n\\nWhat's the highest possible value for the second number? (Note: your code must work for arbitrary integers).\\n\\nWhat's the largest possible value for the second number?\\n\\nThe following number should not be the answer: 2100 or 45254\"], [\" How can you use multiple numbers in a single equation to reach a target sum? To create an equation using the numbers [14, 6, 10] that equals 46, we need to rearrange the digits to fit the equation. Here's one way to do it:\\n\\n1. **Combine the largest two-digit number with the remaining two-digit number:**\\n   - The largest two-digit number is 14 and the remaining two-digit number is 10.\\n   - Adding these together gives us: \\\\(14 + 10 = 24\\\\).\\n\\n2. **Rewrite the equation to\", \" To create an equation using the numbers 14, 6, and 10 that equals 46, we need to place 14 and 6 in different positions (or in pairs) to balance each other out. Here's one possible way to do it:\\n\\n\\\\[ 10 \\\\cdot 14 + 6 \\\\cdot 10 = 46 \\\\]\\n\\nThis equation uses the numbers 10 and 6 twice and gives the result of 46.\\n\\nThe box is correct because it appears in the equation, but it is not explicitly marked. The correct and unmarked box should be the\", \" To create an equation that sums up to 46 using the given numbers [14, 6, 10], we need to find a combination of addition that equals 46. Here's one way to break it down:\\n\\n1. Start with the numbers 10, 14, and 6.\\n2. Try adding another number to see if we can form a sum of 46.\\n\\nLet's do this step by step:\\n\\n- Adding 14 to 6 gives us 20, which is much less than 46.\\n- So, let's try adding 20 to\"]]\n"
     ]
    }
   ],
   "source": [
    "# Use grpo sample outputs function\n",
    "outputs, logprobs = sample_outputs(\n",
    "    policy=model,\n",
    "    tokenizer=tokenizer,\n",
    "    d_b=smaller_batch,\n",
    "    G=3\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reasoning-decomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
