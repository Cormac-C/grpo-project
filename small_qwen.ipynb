{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66e2204a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5df20673",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cormaccureton/mambaforge/envs/grpo-proj/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "from dataset.countdown_dataloader import Countdown\n",
    "from dataset.countdown_utils import ( gen_dataset, compute_metrics, batch_compute_metrics )\n",
    "from grpo import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fba54f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f29d9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5edcef81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and save a tiny dataset with 5 samples\n",
    "dataset_json_path = \"simpler_countdown_data.json\"\n",
    "gen_dataset(num_samples=5, num_operands=3, max_target=100, max_number=15, save_path=dataset_json_path)\n",
    "\n",
    "# Load the dataset\n",
    "dataset = Countdown(dataset_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9610e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Initialize the model with empty weights if needed\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c98b7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch out dataset\n",
    "batch_size = 3\n",
    "batch_raw = dataset.get_batch(batch_size)\n",
    "\n",
    "# Combine whole dataset into prompts\n",
    "batch = [\n",
    "  f\"Using the numbers {item[\"numbers\"]}, create an equation that equals {item[\"target\"]}. Box your answer.\" \n",
    "  for item in batch_raw\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "154f5a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "2025-04-07 11:13:20,987 - INFO - Generated IDs shape: torch.Size([9, 512])\n",
      "2025-04-07 11:13:20,999 - INFO - Responses shape: 3, 3\n",
      "2025-04-07 11:13:21,000 - INFO - Generated IDs reshaped: torch.Size([3, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "# Use grpo sample outputs function\n",
    "outputs_ids, outputs = sample_outputs(\n",
    "    policy=model,\n",
    "    tokenizer=tokenizer,\n",
    "    query_batch=batch,\n",
    "    G=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7b40a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 11:13:21,121 - INFO - Rewards tensor shape: torch.Size([3, 3])\n",
      "2025-04-07 11:13:21,122 - INFO - Accuracies tensor shape: torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "# Calculate rewards for outputs\n",
    "rewards, accuracies = batch_compute_metrics(\n",
    "    outputs,\n",
    "    queries=batch_raw\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02b8d86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Using the numbers [1, 2, 4], create an equation that equals 5. Box your answer.\n",
      "Output: ['Human beings should not be asked to guess.  \\n[1, 1, 2, 2, 2, 4, 4, 4]\\n[1, 4, 4, 4, 4, 4, 4, 4]\\n[2, 1, 2, 1, 1, 4, 4, 4]\\n[1, 1, 4, 2, 2, 4, 4, 4]', \"If you haven't already, don't change your calculator to scientific mode.\\nTo create an equation that equals 5 using the numbers [1, 2, 4] and the rules of solving for an unknown variable, let's denote the unknown variable as \\\\( x \\\\).\\n\\nHere's the equation you could create:\\n\\\\[ x + 4 - 2 + 1 = 5 \\\\]\\n\\nLet's break it down step-by-step:\\n\\n1. Start with the sum of the given numbers and subtract 2.\\n2. Add 1 and divide by 4 to isolate \\\\( x \\\\).\\n\\nNow, let's verify this by plugging in the numbers to see if we get 5.\\n\\n### Verification\\n\\\\[ x + 4 - 2 + 1 = 5 \\\\]\\n\\\\[ x + 3 = 5 \\\\]\\n\\\\[ x = 2 \\\\]\\n\\nThe solution \\\\( x = 2 \\\\) satisfies the equation, so the equation \\\\( x + 4 - 2 + 1 = 5 \\\\) works.\\n\\nSo, an equation that equals 5 using the numbers [1, 2, 4] is:\\n\\\\[ \\\\boxed{x + 4 - 2 + 1 = 5} \\\\]\\n\\n### Step-by-Step Explanation\\n1. Start with the sum of the given numbers: \\\\( (1 + 2 + 4) = 7 \\\\).\\n2. Subtract 2: \\\\( 7 - 2 = 5 \\\\).\\n3. Add 1: \\\\( 5 + 1 = 6 \\\\). However, this doesn't match the original equation exactly.\\n4. Correct the calculation: \\\\( 5 + 1 + 2 - 4 = 6 \\\\), which doesn't work either.\\n5. Correct again by adding 1 and dividing by 4: \\\\( 6 / 4 = 1.5 \\\\), but this doesn't fit the equation as it stands.\\n6. Correct again by dividing by the correct value: \\\\( 6 / 4 = 1.5 \\\\), but this still doesn't fit.\\n7. Correct again by adding 2 and adding 1: \\\\( 6 + 2 + 1 = 9 \\\\), which isn't likely to equal 5.\\n8. Correct by isolating \\\\( x \\\\) by subtracting 1 from the equation: \\\\( 6 - 1 = 5 \\\\).\\n9. Correct by dividing by 4 as a new value: \\\\( 5 / 4\", \"A) 12+14+4-2=B) 12-2+4+1100-4/5=-70/5-100/3\\nB)\\n\\nTo create an equation that equals 5 using the numbers 1, 2, and 4, you need to find a combination that satisfies the given conditions:\\n\\nA) 12 + 14 + 4 - 2\\nB) 12 - 2 + 4 + 1100 - 4 / 5 = -70 / 5 - 100 / 3\\n\\nLet's solve each inequality step by step.\\n\\n**For inequality A:**\\n1. Set up the equation:\\n   12 + 14 + 4 - 2 = 24\\n\\n2. Check the conditions:\\n   - Add 12 and 14 to get 26.\\n   - Add 4 to get 26.\\n   - Subtract 2 to get 24.\\n\\nSo, inequality A is true.\\n\\n**For inequality B:**\\n1. Find the common factors of 12, 12, 4, and 4:\\n   - 12 is divisible by 12.\\n   - 4 is divisible by 4.\\n   - All numbers are equal, so they are not different.\\n\\n2. Check the conditions:\\n   - 12 - 2 = 10\\n   - 12 - 2 + 4 = 14\\n   - 4 / 5 = 0.8\\n   - 1100 - 4 / 5 = 1096.0\\n\\n   Since none of the conditions are satisfied, inequality B is false.\\n\\nTherefore, the only valid equality is for inequality A, which equals 24. The correct answer is:\\n\\n**A) 12 + 14 + 4 - 2 = 24.**\\n\\nWe are almost done, but we have a mistake in the calculation for inequality B. Let's correct it:\\n\\n1. Find the common factors of 12, 12, 4, and 4:\\n   - 12 is divisible by 12.\\n   - 4 is divisible by 4.\\n\\n12 - 2 = 10\\n12 - 2 + 4 = 14\\n4 / 5 = 0.\"]\n",
      "Reward: tensor([0.0000, 0.1000, 0.1000])\n",
      "Accuracy: tensor([0., 0., 0.])\n",
      "--------------------\n",
      "Input: Using the numbers [14, 6, 10], create an equation that equals 46. Box your answer.\n",
      "Output: [' 46 + (6 + 10) = 46.\\n46 + (6 + 10) = 46.\\nThe given numbers 14, 6, 10 are already given in the problem as per the format. So, the equation that equals 46 is already valid and provided.\\n\\nSo, the answer is:\\n\\\\[ \\\\boxed{46 + (6 + 10) = 46} \\\\]', \" To find an equation that equals 46 using the numbers [14, 6, 10], you can try multiple combinations. Since the numbers are larger than 4, let's start with one of these:\\n\\nAssuming \\\\(6\\\\) and \\\\(10\\\\) are large enough (for practical purposes), let's try \\\\(4 \\\\times 6 = 24\\\\) and then add another term. However, if our numbers are reasonably small, we can simplify this by considering some other combinations.\\n\\nLet's try another approach or find a simple equation that fits the criteria. We should look for a combination that adds three terms and that is easy to check. Here are a few possible combinations:\\n\\n1. \\\\(4 + 6 + 6 = 16\\\\) but it's not close to 46.\\n2. \\\\(4 + 6 + 10 = 20\\\\)\\n3. \\\\(4 + 10 + 6 = 20\\\\)\\n4. \\\\(6 + 4 + 10 = 20\\\\) but again not 46.\\n5. \\\\(10 + 6 + 4 = 20\\\\) but again not 46.\\n\\nOne possible equation that works is 4, 6, and 10:\\n\\\\[4 \\\\times 6 + 10 = 44 + 10 = 54 - 8 = 46\\\\]\\n\\nHowever, we need to confirm it meets the criteria. Let's check:\\n\\n1. Check: \\\\(14 + 6 + 10 = 20 + 10 = 40 - 8 = 32 - 5 = 37 - 5 = 32 \\\\times 2 = 64 - 16 = 48 \\\\neq 46\\\\)\\n\\nIt seems we need more careful selection. Let's try another possible combination:\\n\\n6, 10, 14:\\n\\\\[6 + 10 + 14 = 6 + 14 + 10 = 6 \\\\times 14 = 84 - 6 = 78 - 1 = 77 - 4 = 73 - 6 = 67 - 11 = 66 - 7 = 60 \\\\times 11 \\\\neq 46\\\\]\\n\\nWe need to reconsider. The correct combination is:\\n\\\\[4 \\\\times 6 \\\\\", ' **Equation:**\\n    **46 = [14] + [6] - [10]**\\nThis equation utilizes the basic arithmetic rules, where we sum up the integers 14, 6, and 10 to get 40. Subtracting this total (40) from 46 yields the desired sum. The answer is 46.']\n",
      "Reward: tensor([0.1000, 0.1000, 0.1000])\n",
      "Accuracy: tensor([0., 0., 0.])\n",
      "--------------------\n",
      "Input: Using the numbers [11, 2, 10], create an equation that equals 88. Box your answer.\n",
      "Output: [\" **Given:** The given set of numbers: [11, 2, 10]\\n\\n**Question:** Can you make an equation with the given numbers (11, 2, 10) and get 88?\\n\\n**Solution:**\\n\\nConsider the following equation:\\n\\n```\\n11 + 10 - 2 = 88\\n```\\n\\nNow, let's break down the steps to solve this equation:\\n\\n1. Start with the given numbers: 11, 2, and 10.\\n\\n2. Add the first pair of numbers: 11 + 2 = 13.\\n\\n3. Subtract the second pair of numbers: 13 - 10 = 3.\\n\\n4. Add the third pair of numbers: 3 + 10 = 13.\\n\\n5. Compare the result with the target value of 88. Since 13 is not equal to 88, there is no equation in our given set of numbers that equals 88. \\n\\nTherefore, we cannot create an equation with the given numbers that equals 88.\\n\\n**Answer:** We cannot make an equation with the given numbers (11, 2, 10) and get 88.\", ' 11 + 2 + 10 + (10 * 2) + (11 * 2) + (10 * 2) + (11 * 2) = 88.\\n\\nHow the equation works:\\nIn this equation we are first adding 11, 2 and 10. The sum of 11 + 2 is 13, and adding the next 10 (10 * 2) gives us 26. Then, the previous sum is 13 + 10 which is 23, and when multiplied by 2, equals 46. After multiplying 10 by 2, the result is 20, added to the previous sum and multiplied by 2, and finally multiplied by 2 to get 48.\\nAdding the results 46, 48 and 48 (since there is a + (10 * 2), the last 10 needs to be added once more to get 88.', \" To create an equation using the numbers \\\\([11, 2, 10]\\\\) that equal 88, we need to find a combination of these values that adds up to 88. Let's try to arrange the numbers in different ways to see if we can form the exact equation.\\n\\nOne way to do this is to consider the equation as a sum of four numbers, each being a factor of 88. The factors of 88 are: 1, 2, 4, 8, 11, 22, 44, and 88. However, we only need four factors, so we will try to use all five of them or arrange them in such a way that we can form the exact equation.\\n\\nA common approach is to try to use the smallest possible numbers first and see if we can find a combination. Let's start by trying to create a pair of factors with the smallest values, and then see if we can make the other numbers using the remaining factors.\\n\\nOne possible combination is to use the numbers 1, 11, 2, 10. Adding them together, we get:\\n\\n\\\\[1 + 11 + 2 + 10 = 24\\\\]\\n\\nWe need to add 8 to 24 to get 112, but 10 is also needed. So, let's try another combination:\\n\\nThe numbers 1, 11, 2, 10. Adding them together again, we get:\\n\\n\\\\[1 + 11 + 2 + 10 = 24\\\\]\\n\\nWe need to add 8 to 24 to get 112, but 10 is also needed. Let's try another combination with the same numbers:\\n\\nThe numbers 1, 11, 2, 10. Adding them together again, we get:\\n\\n\\\\[1 + 11 + 2 + 10 = 24\\\\]\\n\\nWe need to add 8 to 24 to get 112, but 10 is also needed. We can try another combination with the same numbers:\\n\\nThe numbers 1, 11, 2, 10. Adding them together again, we get:\\n\\n\\\\[1 + 11 + 2 + 10 = 24\\\\]\\n\\nWe need to add 8 to 24 to get 112, but 1\"]\n",
      "Reward: tensor([0.1000, 0.1000, 0.1000])\n",
      "Accuracy: tensor([0., 0., 0.])\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# Print the outputs and rewards\n",
    "for i, output in enumerate(outputs):\n",
    "    print(f\"Input: {batch[i]}\")\n",
    "    print(f\"Output: {output}\")\n",
    "    print(f\"Reward: {rewards[i]}\")\n",
    "    print(f\"Accuracy: {accuracies[i]}\")\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03527a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 11:13:21,249 - INFO - Advantages shape: torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "# Compute GRPO advantage\n",
    "advantage = calculate_grpo_advantage(\n",
    "    rewards\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "242e731e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advantage: tensor([[-1.1547,  0.5774,  0.5774],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Advantage: {advantage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa5082b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 11:13:21,298 - INFO - Query IDs shape: torch.Size([3, 3, 27])\n",
      "2025-04-07 11:13:21,298 - INFO - Generated IDs shape: torch.Size([3, 3, 512])\n",
      "2025-04-07 11:13:21,299 - INFO - Input IDs shape: torch.Size([3, 3, 539])\n",
      "2025-04-07 11:13:21,299 - INFO - Reshaped Input IDs shape: torch.Size([9, 539])\n",
      "2025-04-07 11:13:21,299 - INFO - Attention mask shape: torch.Size([9, 539])\n",
      "2025-04-07 11:14:34,465 - INFO - Logits shape: torch.Size([9, 539, 151936])\n",
      "2025-04-07 11:14:34,742 - INFO - Generated logits shape: torch.Size([9, 512, 151936])\n",
      "2025-04-07 11:14:38,196 - INFO - Log probabilities shape: torch.Size([9, 512, 151936])\n",
      "2025-04-07 11:14:38,423 - INFO - Gathered log probabilities shape: torch.Size([9, 512])\n",
      "2025-04-07 11:14:38,446 - INFO - Reshaped log probabilities shape: torch.Size([3, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "# Compute log probabilities\n",
    "log_probs = compute_log_probs(\n",
    "    policy=model,\n",
    "    tokenizer=tokenizer,\n",
    "    query_batch=batch,\n",
    "    generated_ids=outputs_ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ecc2c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 11:14:41,787 - INFO - Prob ratios shape: torch.Size([3, 3, 512])\n",
      "2025-04-07 11:14:41,823 - INFO - Clipped ratios shape: torch.Size([3, 3, 512])\n",
      "2025-04-07 11:14:41,823 - INFO - Advantages shape: torch.Size([3, 3, 1])\n",
      "2025-04-07 11:14:41,885 - INFO - Min product shape: torch.Size([3, 3, 512])\n",
      "2025-04-07 11:14:41,889 - INFO - KL divergence shape: torch.Size([3, 3, 512])\n",
      "2025-04-07 11:14:41,892 - INFO - Objective shape: torch.Size([3, 3, 512])\n",
      "2025-04-07 11:14:41,933 - INFO - Final objective shape: torch.Size([3])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRPO Objective: tensor([-7.9473e-08,  0.0000e+00,  0.0000e+00], grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Calculate GRPO objective\n",
    "grpo_objective = calculate_grpo_objective(\n",
    "    model_log_probs=log_probs,\n",
    "    old_model_log_probs=log_probs,  # Assuming old model is the same for this example\n",
    "    ref_model_log_probs=log_probs,  # Assuming ref model is the same for this example\n",
    "    advantages=advantage,\n",
    "    eps=0.1,  # Epsilon for clipping\n",
    "    beta=0.05,  # Beta for the objective function\n",
    ")\n",
    "\n",
    "print(f\"GRPO Objective: {grpo_objective}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c99518bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "722e35d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "2025-04-07 11:17:34,076 - INFO - Generated IDs shape: torch.Size([9, 512])\n",
      "2025-04-07 11:17:34,114 - INFO - Responses shape: 3, 3\n",
      "2025-04-07 11:17:34,114 - INFO - Generated IDs reshaped: torch.Size([3, 3, 512])\n",
      "2025-04-07 11:17:34,121 - INFO - Rewards tensor shape: torch.Size([3, 3])\n",
      "2025-04-07 11:17:34,122 - INFO - Accuracies tensor shape: torch.Size([3, 3])\n",
      "2025-04-07 11:17:34,124 - INFO - Advantages shape: torch.Size([3, 3])\n",
      "2025-04-07 11:17:34,133 - INFO - Query IDs shape: torch.Size([3, 3, 27])\n",
      "2025-04-07 11:17:34,134 - INFO - Generated IDs shape: torch.Size([3, 3, 512])\n",
      "2025-04-07 11:17:34,134 - INFO - Input IDs shape: torch.Size([3, 3, 539])\n",
      "2025-04-07 11:17:34,134 - INFO - Reshaped Input IDs shape: torch.Size([9, 539])\n",
      "2025-04-07 11:17:34,136 - INFO - Attention mask shape: torch.Size([9, 539])\n",
      "2025-04-07 11:32:18,831 - INFO - Logits shape: torch.Size([9, 539, 151936])\n",
      "2025-04-07 11:32:18,868 - INFO - Generated logits shape: torch.Size([9, 512, 151936])\n",
      "2025-04-07 11:32:22,313 - INFO - Log probabilities shape: torch.Size([9, 512, 151936])\n",
      "2025-04-07 11:32:22,428 - INFO - Gathered log probabilities shape: torch.Size([9, 512])\n",
      "2025-04-07 11:32:22,433 - INFO - Reshaped log probabilities shape: torch.Size([3, 3, 512])\n",
      "2025-04-07 11:32:23,051 - INFO - Query IDs shape: torch.Size([3, 3, 27])\n",
      "2025-04-07 11:32:23,056 - INFO - Generated IDs shape: torch.Size([3, 3, 512])\n",
      "2025-04-07 11:32:23,082 - INFO - Input IDs shape: torch.Size([3, 3, 539])\n",
      "2025-04-07 11:32:23,082 - INFO - Reshaped Input IDs shape: torch.Size([9, 539])\n",
      "2025-04-07 11:32:23,164 - INFO - Attention mask shape: torch.Size([9, 539])\n",
      "2025-04-07 11:34:49,119 - INFO - Logits shape: torch.Size([9, 539, 151936])\n",
      "2025-04-07 11:34:49,265 - INFO - Generated logits shape: torch.Size([9, 512, 151936])\n",
      "2025-04-07 11:34:55,046 - INFO - Log probabilities shape: torch.Size([9, 512, 151936])\n",
      "2025-04-07 11:34:56,199 - INFO - Gathered log probabilities shape: torch.Size([9, 512])\n",
      "2025-04-07 11:34:56,204 - INFO - Reshaped log probabilities shape: torch.Size([3, 3, 512])\n",
      "2025-04-07 11:34:56,885 - INFO - Prob ratios shape: torch.Size([3, 3, 512])\n",
      "2025-04-07 11:34:56,963 - INFO - Clipped ratios shape: torch.Size([3, 3, 512])\n",
      "2025-04-07 11:34:56,974 - INFO - Advantages shape: torch.Size([3, 3, 1])\n",
      "2025-04-07 11:34:57,060 - INFO - Min product shape: torch.Size([3, 3, 512])\n",
      "2025-04-07 11:34:57,065 - INFO - KL divergence shape: torch.Size([3, 3, 512])\n",
      "2025-04-07 11:34:57,070 - INFO - Objective shape: torch.Size([3, 3, 512])\n",
      "2025-04-07 11:34:57,179 - INFO - Final objective shape: torch.Size([3])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m updated_policy = \u001b[43mgrpo_iteration\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_batch_prompts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_batch_raw\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpolicy_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreference_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Assuming reference model is the same for this example\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreward_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_compute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mG\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Epsilon for clipping\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Beta for the objective function\u001b[39;49;00m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmu\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\n\u001b[32m     13\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/grpo-project/grpo.py:97\u001b[39m, in \u001b[36mgrpo_iteration\u001b[39m\u001b[34m(query_batch_prompts, query_batch_raw, policy_model, reference_model, reward_model, tokenizer, optimizer, G, eps, beta, mu)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# Compute gradient of the GRPO objective\u001b[39;00m\n\u001b[32m     96\u001b[39m loss = -objective\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[38;5;66;03m# Update the policy\u001b[39;00m\n\u001b[32m    100\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mambaforge/envs/grpo-proj/lib/python3.13/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mambaforge/envs/grpo-proj/lib/python3.13/site-packages/torch/autograd/__init__.py:340\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    331\u001b[39m inputs = (\n\u001b[32m    332\u001b[39m     (inputs,)\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, (torch.Tensor, graph.GradientEdge))\n\u001b[32m   (...)\u001b[39m\u001b[32m    336\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m ()\n\u001b[32m    337\u001b[39m )\n\u001b[32m    339\u001b[39m grad_tensors_ = _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m grad_tensors_ = \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mambaforge/envs/grpo-proj/lib/python3.13/site-packages/torch/autograd/__init__.py:198\u001b[39m, in \u001b[36m_make_grads\u001b[39m\u001b[34m(outputs, grads, is_grads_batched)\u001b[39m\n\u001b[32m    196\u001b[39m     out_numel_is_1 = out.numel() == \u001b[32m1\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out_numel_is_1:\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    199\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    200\u001b[39m     )\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out_dtype.is_floating_point:\n\u001b[32m    202\u001b[39m     msg = (\n\u001b[32m    203\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mgrad can be implicitly created only for real scalar outputs\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    204\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    205\u001b[39m     )\n",
      "\u001b[31mRuntimeError\u001b[39m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "updated_policy = grpo_iteration(\n",
    "    query_batch_prompts=batch,\n",
    "    query_batch_raw=batch_raw,\n",
    "    policy_model=model,\n",
    "    reference_model=model,  # Assuming reference model is the same for this example\n",
    "    reward_model=batch_compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    optimizer=optimizer,\n",
    "    G=3,\n",
    "    eps=0.1,  # Epsilon for clipping\n",
    "    beta=0.05,  # Beta for the objective function\n",
    "    mu=3\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grpo-proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
